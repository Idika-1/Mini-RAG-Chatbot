{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5f049ab-2506-493c-8a8e-4c73b94b6158",
   "metadata": {},
   "source": [
    "**<h4>Author: IDIKA, UDUMA UDUMA</h4>**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17fe4a2d-262d-4b6d-98a1-812ecb244105",
   "metadata": {},
   "source": [
    "**<h4>AI Project: Mini-RAG Chatbot System</h4>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96056594-741a-401e-b7a6-0499c5a5ba54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\rag_env\\Scripts\\python.exe\n"
     ]
    }
   ],
   "source": [
    "#Confirming that this project is executed in my virtual environment (rag_env)\n",
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f507edfd-f1f0-4665-9804-fa20c07edf67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a pip_install function to neatly install the required packages in the virtual environment\n",
    "def pip_install(package):\n",
    "    import sys\n",
    "    import subprocess\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "548bc523-7327-46b2-851c-ad34d978a983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing 'sentence-transformers'...\n",
      "Installation of 'sentence-transformers' completed✅\n",
      "\n",
      "Installing 'faiss-cpu'...\n",
      "Installation of 'faiss-cpu' completed✅\n",
      "\n",
      "Installing 'pdfplumber'...\n",
      "Installation of 'pdfplumber' completed✅\n",
      "\n",
      "Installing 'python-docx'...\n",
      "Installation of 'python-docx' completed✅\n",
      "\n",
      "Installing 'transformers'...\n",
      "Installation of 'transformers' completed✅\n",
      "\n",
      "Installing 'accelerate'...\n",
      "Installation of 'accelerate' completed✅\n",
      "\n",
      "Installing 'torch'...\n",
      "Installation of 'torch' completed✅\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Installing the required packages\n",
    "packages = [\"sentence-transformers\", \"faiss-cpu\", \"pdfplumber\", \"python-docx\", \"transformers\", \"accelerate\", \"torch\"]\n",
    "\n",
    "for package in packages:\n",
    "    print(f\"Installing '{package}'...\")\n",
    "    pip_install(package)\n",
    "    print(f\"Installation of '{package}' completed✅\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3715c2-e0c4-4a79-bdff-e912cfac4155",
   "metadata": {},
   "source": [
    "**<h4><u>Steps to be Taken:</u></h4>**\n",
    "> - **Step 1:** Document Upload & Text Extraction\n",
    "> - **Step 2:** Text Cleaning, Preprocessing, and Chunking\n",
    "> - **Step 3:** Embedding Computation\n",
    "> - **Step 4:** Vector Index (FAISS)\n",
    "> - **Step 5:** Retrieval & Prompt Construction\n",
    "> - **Step 6:** Answer Generation with an LLM\n",
    "> - **Step 7:** Simple Interactive Loop (Mini Chatbot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63551786-ac14-4f11-9c57-1c9d22cee7e4",
   "metadata": {},
   "source": [
    "**<h4><u>Step 1:</u> Document Upload & Text Extraction</h4>**\n",
    "> - Create an uploads/ folder for storing temporary uploaded files\n",
    "> - Define a function that accepts a file path\n",
    "> - Automatically determine the file type (.pdf, .docx, .txt)\n",
    "> - Extract text from the document using the right parser:<br>\n",
    "         * PDF: pdfplumber<br>\n",
    "         * DOCX: python-docx<br>\n",
    "         * TXT: simple open()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75e8ba4a-ff03-4f91-8a7a-524203c93592",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the required libraries\n",
    "import os\n",
    "import pdfplumber\n",
    "from docx import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ac92b42-d5c3-473c-b2c2-92b630f34ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------\n",
    "# Creating an upload directory\n",
    "# ----------------------------------\n",
    "UPLOAD_DIR = \"uploads\"\n",
    "os.makedirs(UPLOAD_DIR, exist_ok=True)\n",
    "\n",
    "# ----------------------------------\n",
    "# Function: Extracting text from PDF\n",
    "# ----------------------------------\n",
    "def extract_text_from_pdf(file_path):\n",
    "    text = []\n",
    "    with pdfplumber.open(file_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            content = page.extract_text()\n",
    "            if content:\n",
    "                text.append(content)\n",
    "    return \"\\n\".join(text)\n",
    "\n",
    "\n",
    "# ----------------------------------\n",
    "# Function: Extracting text from DOCX\n",
    "# ----------------------------------\n",
    "def extract_text_from_docx(file_path):\n",
    "    doc = Document(file_path)\n",
    "    return \"\\n\".join([p.text for p in doc.paragraphs])\n",
    "\n",
    "\n",
    "# ----------------------------------\n",
    "# Function: Extracting text from TXT\n",
    "# ----------------------------------\n",
    "def extract_text_from_txt(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        return f.read()\n",
    "\n",
    "\n",
    "# ----------------------------------\n",
    "# File type detection \n",
    "# ----------------------------------\n",
    "def detect_file_type(file_path):\n",
    "    #1. Try using the file extension\n",
    "    ext = os.path.splitext(file_path)[1].lower()\n",
    "    if ext in [\".pdf\", \".docx\", \".txt\"]:\n",
    "        return ext\n",
    "\n",
    "    #2. Peeking into the file signature (magic bytes)\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        header = f.read(4)\n",
    "\n",
    "    if header.startswith(b\"%PDF\"):\n",
    "        return \".pdf\"\n",
    "\n",
    "    elif header.startswith(b\"PK\"):  #DOCX files are ZIP containers\n",
    "        return \".docx\"\n",
    "\n",
    "    else:\n",
    "        #Deafult fallback: assume text\n",
    "        return \".txt\"\n",
    "    \n",
    "\n",
    "# ----------------------------------\n",
    "# Main Extraction Function \n",
    "# ----------------------------------\n",
    "def extract_text(file_path):\n",
    "    \"\"\"\n",
    "    This function extracts text from a document (PDF, DOCX, or TXT).\n",
    "    It automatically detects the file type by extension.\n",
    "    \"\"\"\n",
    "    file_type = detect_file_type(file_path)\n",
    "    \n",
    "    if file_type == \".pdf\":         #Call the pdf extractor\n",
    "        return extract_text_from_pdf(file_path)\n",
    "\n",
    "    elif file_type == \".docx\":     #Call the docx extractor\n",
    "        return extract_text_from_docx(file_path)\n",
    "\n",
    "    elif file_type == \".txt\":      #Call the txt extractor\n",
    "        return extract_text_from_txt(file_path)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"❌Unsupported file type: '{ext}'\\n Only .pdf, .docx, or .txt files are supported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b31320e2-df43-4e8c-a713-04d435f8ba33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8a26e282bdc4453b8dfcee015c61453",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FileUpload(value=(), accept='.pdf,.docx,.txt', description='Upload File')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c691fad96504cac90e81bda5d0b52d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='success', description='Process File', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a15982f9dd614f269413ef72466647b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ----------------------------------\n",
    "# Interactive File Upload + Process\n",
    "# ----------------------------------\n",
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "import shutil\n",
    "\n",
    "upload_widget = widgets.FileUpload(\n",
    "    accept='.pdf,.docx,.txt',\n",
    "    multiple=False,             #Allows only single file upload\n",
    "    description='Upload File'\n",
    ")\n",
    "\n",
    "process_button = widgets.Button(description=\"Process File\", button_style='success')\n",
    "output_area = widgets.Output()\n",
    "\n",
    "# Global storage for access after processing\n",
    "uploaded_file_path = None\n",
    "extracted_text = None\n",
    "\n",
    "def on_process_clicked(b):\n",
    "    global uploaded_file_path, extracted_text   #Declaring global variables\n",
    "    \n",
    "    if not upload_widget.value:\n",
    "        with output_area:\n",
    "            output_area.clear_output()\n",
    "            print(\"⚠ Please upload a file first.\")\n",
    "        return\n",
    "\n",
    "    #Support both dict and tuple return formats\n",
    "    upload_value = upload_widget.value\n",
    "    if isinstance(upload_value, dict):\n",
    "        uploaded_file = list(upload_value.values())[0]\n",
    "    elif isinstance(upload_value, tuple):\n",
    "        uploaded_file = upload_value[0]\n",
    "    else:\n",
    "        with output_area:\n",
    "            output_area.clear_output()\n",
    "            print(\"❌ Unexpected upload format:\", type(upload_value))\n",
    "        return\n",
    "\n",
    "    #Get uploaded file data\n",
    "    filename = uploaded_file.get(\"metadata\", {}).get(\"name\", \"uploaded_file\")\n",
    "    file_path = os.path.join(UPLOAD_DIR, filename)\n",
    "\n",
    "    #Save uploaded content to disk\n",
    "    with open(file_path, \"wb\") as f:\n",
    "        f.write(uploaded_file[\"content\"])\n",
    "\n",
    "    #Extract text\n",
    "    text = extract_text(file_path)\n",
    "\n",
    "    #Store results in global variables\n",
    "    uploaded_file_path = file_path\n",
    "    extracted_text = text\n",
    "\n",
    "    with output_area:\n",
    "        output_area.clear_output()\n",
    "        print(f\"✅ File uploaded and saved to: {file_path}\")\n",
    "        print(f\"📄 Extracted {len(text)} characters of text.\\n\")\n",
    "        print(\"Preview of first few characters:\")\n",
    "        print(text[:200], \"...\\n\")    #Preview of first 200 characters\n",
    "\n",
    "        \n",
    "process_button.on_click(on_process_clicked)\n",
    "\n",
    "display(upload_widget, process_button, output_area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "815fcce9-e098-4954-b562-8616793de964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ File upload confirmed. Proceeding...\n"
     ]
    }
   ],
   "source": [
    "#Asserting that a file was uploaded and processed\n",
    "assert uploaded_file_path is not None, \"⚠ You must upload and process a file before proceeding!\"\n",
    "\n",
    "print(\"✅ File upload confirmed. Proceeding...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9c6be9-f022-4564-9346-4a1e966536c3",
   "metadata": {},
   "source": [
    "**<h4><u>Step 2:</u> Text Cleaning, Preprocessing and Chunking**</h4>\n",
    "> - Clean and normalize the extracted text.\n",
    "> - Split text into overlapping chunks for better context continuity\n",
    "> - Display sample chunks to verify the process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2c466139-2288-4131-be7f-811ea31c0b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re    #regular expression (regex) library\n",
    "# ----------------------------------\n",
    "# Text Cleaning Function\n",
    "# ----------------------------------\n",
    "def clean_text(text):\n",
    "    \"\"\"Normalize and clean extracted text.\"\"\"\n",
    "    #Normalizing line breaks and spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)  #Replaces multiple spaces/newlines with one space\n",
    "\n",
    "    #Removing continuous dot leaders (e.g., \"Introduction ..... 3\")\n",
    "    text = re.sub(r'\\.{3,}\\s*\\d*', ' ', text)\n",
    "\n",
    "    #Removing spaced dot leaders\n",
    "    text = re.sub(r'(\\.\\s){2,}\\d*', ' ', text)\n",
    "\n",
    "    #Removing isolated page numbers (e.g., \"Page 2\", \"2\")\n",
    "    text = re.sub(r'\\bPage\\s*\\d+\\b', ' ', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\b\\d+\\b', ' ', text)     #Removing stray digits\n",
    "\n",
    "    #Removing non-ASCII characters\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)  #Removes non-ASCII characters\n",
    "  \n",
    "    #Normalizing multiple spaces again\n",
    "    text = re.sub(r'\\s{2,}', ' ', text)\n",
    "\n",
    "    #Trimming final whitespaces\n",
    "    text = text.strip()\n",
    "\n",
    "    #Return the cleaned text\n",
    "    return text\n",
    "\n",
    "# ----------------------------------\n",
    "# Text Chunking Function\n",
    "# ----------------------------------\n",
    "def chunk_text(text, chunk_size=1000, overlap=100):\n",
    "    \"\"\"\n",
    "    This function splits text into overlapping chunks\n",
    "    :param text: Cleaned text.\n",
    "    :param chunk_size: Number of characters per chunk\n",
    "    :param overlap: Overlap between chunks to maintain context\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    text_length = len(text)\n",
    "\n",
    "    while start < text_length:\n",
    "        end = min(start + chunk_size, text_length)\n",
    "        chunk = text[start:end]\n",
    "        chunks.append(chunk)\n",
    "        start += chunk_size - overlap  #Redefining the start for the next iteration\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fa9331c4-149c-44c6-9b1a-2226d9eaa001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Cleaned text length: 1269 characters\n",
      "✅ Created 2 chunks (avg 1000 chars each)\n",
      "\n",
      "--- Chunk 1 ---\n",
      "Reference (the registration number from the file management system for foreign nationals in France; this must be provided in any correspondence): Once your VLS-TS long stay visa is validated, you are  ...\n",
      "\n",
      "--- Chunk 2 ---\n",
      "/ / , you validated your VLS-TS long stay visa and paid the fees for your initial residence permit: Type: ETUDIANT Regulatory reference number: CESEDA R311- Fee amount: . Following this process, the f ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------\n",
    "# Example Run After Extraction\n",
    "# ----------------------------------\n",
    "def process_extracted_text(file_path):\n",
    "    cleaned = clean_text(extracted_text)\n",
    "    chunks = chunk_text(cleaned)\n",
    "\n",
    "    print(f\"✅ Cleaned text length: {len(cleaned)} characters\")\n",
    "    print(f\"✅ Created {len(chunks)} chunks (avg {len(chunks[0])} chars each)\\n\")\n",
    "\n",
    "    #Preview first 2 chunks\n",
    "    for i, c in enumerate(chunks[:2]):\n",
    "        print(f\"--- Chunk {i+1} ---\")\n",
    "        print(c[:200], \"...\\n\")\n",
    "\n",
    "    return chunks\n",
    "\n",
    "chunks = process_extracted_text(uploaded_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7adbde48-7638-4b06-8fac-833be13acc91",
   "metadata": {},
   "source": [
    "**<h4><u>Step 3:</u> Creating Embeddings and Building a Vector Index</h4>**\n",
    "> - Utilize a Sentence Transformer model to create the embeddings\n",
    "> - Store the embeddings in a vector search index (FAISS)\n",
    "> - Find the most semantically relevant chunks to a user query using a similarity search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8bff7be0-066b-4a77-9414-b48567e8d95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the relevant libraries\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "718a5def-1549-4128-90f6-9c8a33801646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------\n",
    "# Loading pre-trained embedding model\n",
    "# ----------------------------------\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# ----------------------------------\n",
    "# Creating embeddings from chunks\n",
    "# ----------------------------------\n",
    "def create_embeddings(chunks):\n",
    "    print(\"🔍 Generating embeddings for all chunks...\")\n",
    "    embeddings = embedding_model.encode(chunks, show_progress_bar=True)\n",
    "    return np.array(embeddings).astype(\"float32\")\n",
    "\n",
    "# ----------------------------------\n",
    "# Normalize Function\n",
    "# ----------------------------------\n",
    "def normalize(vectors):\n",
    "    \"\"\"\n",
    "    Normalizing vectors to unit length for cosine similarity\n",
    "    \"\"\"\n",
    "    norms = np.linalg.norm(vectors, axis=1, keepdims=True)\n",
    "    norms[norms == 0] = 1e-10   #Avoiding zero division error\n",
    "    return vectors/norms\n",
    "\n",
    "\n",
    "# ----------------------------------\n",
    "# Process embedding & creating FAISS vector index\n",
    "# ----------------------------------\n",
    "def build_faiss_index(chunks, embedding_model):\n",
    "    \"\"\"\n",
    "    Given text chunks and embedding model, creating a FAISS index using cosine similarity\n",
    "    \"\"\"\n",
    "\n",
    "    #Encoding chunks\n",
    "    embeddings = embedding_model.encode(chunks)\n",
    "\n",
    "    #Normalizing embeddings\n",
    "    embeddings_norm = normalize(embeddings)\n",
    "\n",
    "    #Creating Index using Inner Product\n",
    "    index = faiss.IndexFlatIP(embeddings.shape[1])            # IP = Inner Product\n",
    "    index.add(np.array(embeddings_norm, dtype=np.float32))\n",
    "    \n",
    "    dim = embeddings.shape[1]     #embedding dimension\n",
    "    print(f\"✅ FAISS index built with {index.ntotal} vectors of dim {dim}\")\n",
    "    return index, embeddings_norm\n",
    "\n",
    "# ----------------------------------\n",
    "# Saving and loading index for reuse\n",
    "# ----------------------------------\n",
    "def save_index(index, chunks, path=\"vector_store\"):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    faiss.write_index(index, os.path.join(path, \"index.faiss\"))\n",
    "\n",
    "    with open(os.path.join(path, \"chunks.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(chunks, f)\n",
    "\n",
    "    print(\"💾 Vector store saved successfully.\")\n",
    "\n",
    "def load_index(path=\"vector_store\"):\n",
    "    index = faiss.read_index(os.path.join(path, \"index.faiss\"))\n",
    "    with open(os.path.join(path, \"chunks.pkl\"), \"rb\") as f:\n",
    "        chunks = pickle.load(f)\n",
    "\n",
    "    print(\"📂 Vector store loaded successfully.\")\n",
    "    return index, chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "95a5ded8-f6f0-45df-be0c-5809e893f133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Generating embeddings for all chunks...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1297925778d424e85b02fdfb4932e96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ FAISS index built with 2 vectors of dim 384\n",
      "💾 Vector store saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------\n",
    "# Example test run\n",
    "# ----------------------------------\n",
    "embeddings = create_embeddings(chunks)\n",
    "index, embeddings_norm = build_faiss_index(chunks, embedding_model)\n",
    "save_index(index, chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1442327-40e0-4ae3-9bd4-806d30c495cb",
   "metadata": {},
   "source": [
    "**<h4><u>Step 4:</u></h4>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9cb0fe51-263a-429d-b350-773fe21f57e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hf_xet in c:\\users\\admin\\rag_env\\lib\\site-packages (1.1.10)\n"
     ]
    }
   ],
   "source": [
    "!{sys.executable} -m pip install hf_xet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "89dc140c-ce94-4cd7-929b-aad1f4edc63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "577c6d47-c9f1-42e7-a949-eb6ddcf63073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📂 Vector store loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------\n",
    "# Load the saved FAISS index and chunks\n",
    "# ----------------------------------\n",
    "index, chunks = load_index(\"vector_store\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5d363d83-b53a-4aaa-be7d-345edc7314d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "# Define model and tokenizer names\n",
    "qa_model_name = \"google/flan-t5-base\"\n",
    "qa_tokenizer = AutoTokenizer.from_pretrained(qa_model_name)\n",
    "qa_model = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=qa_model_name,\n",
    "    tokenizer=qa_tokenizer,\n",
    "    device=-1     #Uses CPU\n",
    ")\n",
    "\n",
    "MAX_TOKENS = 512  # FLAN-T5 max input length\n",
    "\n",
    "def trim_context(context, max_tokens=MAX_TOKENS):\n",
    "    \"\"\"\n",
    "    Ensures that the context doesn't exceed the model's max token length.\n",
    "    \"\"\"\n",
    "    tokens = qa_tokenizer.encode(context, truncation=True, max_length=max_tokens)\n",
    "    return qa_tokenizer.decode(tokens, skip_special_tokens=True)\n",
    "\n",
    "\n",
    "def answer_question(query, index, chunks, embedding_model=embedding_model, top_k=2):\n",
    "    \"\"\"\n",
    "    Retrieve relevant chunks, construct prompt, and generate answer using the QA model.\n",
    "    \"\"\"\n",
    "    # Step 1: Encoding and normalizing the user query\n",
    "    query_embedding = embedding_model.encode([query])\n",
    "    query_embedding_norm = normalize(np.array(query_embedding, dtype=np.float32))\n",
    "\n",
    "    # Step 2: Searching FAISS index using cosine similarity and returning only relevant result\n",
    "    distances, indices = index.search(query_embedding_norm, top_k)\n",
    "        \n",
    "    # Step 3: Returning relevant chunks\n",
    "    #Converting to list for easier handling\n",
    "    scores = distances[0]\n",
    "    retrieved_indices = indices[0]\n",
    "\n",
    "    #Setting a similarity threshold\n",
    "    SIMILARITY_THRESHOLD = 0.11        #Lowered value for normalized embeddings\n",
    "\n",
    "    #Filtering out irrelevant chunks\n",
    "    relevant_chunks = [\n",
    "        chunks[i] for i, score in zip(retrieved_indices, scores)\n",
    "        if score >= SIMILARITY_THRESHOLD\n",
    "    ]\n",
    "\n",
    "    #Optional:\n",
    "    print(\"Similarity scores:\", scores)\n",
    "\n",
    "    #If no relevant chunk meets the threshold\n",
    "    if not relevant_chunks:\n",
    "        return \"🤔 I couldn't find relevant information in the uploaded document to answer the question.\"\n",
    "    \n",
    "    # Step 5: Concatenate retrieved chunks into context\n",
    "    context = \" \".join(relevant_chunks)\n",
    "    \n",
    "    # Step 6: Trim context to fit model's token limit\n",
    "    trimmed_context = trim_context(context)\n",
    "    \n",
    "    # Step 7: Create the full prompt\n",
    "    prompt = (\n",
    "        f\"You are a helpful assistant. Use only the information in the context below to answer.\\n\"\n",
    "        f\"Context: {trimmed_context}\\n\\n\"\n",
    "        f\"Question: {query}\\n\\n\"\n",
    "        f\"Answer the question concisely based on the context above.\"\n",
    "        f\"If the answer cannot be found in the context, reply exactly with: \"\n",
    "        f\"'🤔 I couldn't find relevant information in the uploaded document to answer the question.'\"\n",
    "    )\n",
    "    \n",
    "    # Step 8: Generate answer using the model\n",
    "    response = qa_model(\n",
    "        prompt,\n",
    "        max_new_tokens=100,     # safer than max_length\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        do_sample=True\n",
    "    )\n",
    "    \n",
    "    # Step 9: Extract the generated text\n",
    "    answer = response[0][\"generated_text\"].strip()\n",
    "    \n",
    "    # Step 10: Print results\n",
    "    print(f\"\\n🧠 Question: {query}\")\n",
    "    print(f\"\\n💬 Answer:\\n {answer}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "eb01bf15-7efe-49d6-9d97-c7384c5128db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your question:  When is one supposed to complete their medical visit?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity scores: [ 0.3474173  -0.00118358]\n",
      "\n",
      "🧠 Question: When is one supposed to complete their medical visit?\n",
      "\n",
      "💬 Answer:\n",
      " within four months of your arrival in France\n"
     ]
    }
   ],
   "source": [
    "#Testing the solution\n",
    "question = input(\"Enter your question: \")\n",
    "answer_question(question, index, chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b85ba966-4f12-4f13-81b6-053b63ca2d2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* Running on public URL: https://8f5e1191b21fe864b4.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://8f5e1191b21fe864b4.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import os\n",
    "import numpy as np\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "\n",
    "# ----------------------------------\n",
    "# Global State\n",
    "# ----------------------------------\n",
    "state = {\"index\": None, \"chunks\": None}\n",
    "\n",
    "# ----------------------------------\n",
    "# Load Embedding Model\n",
    "# ----------------------------------\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# ----------------------------------\n",
    "# Load QA Model (FLAN-T5)\n",
    "# ----------------------------------\n",
    "qa_model_name = \"google/flan-t5-base\"\n",
    "qa_tokenizer = AutoTokenizer.from_pretrained(qa_model_name)\n",
    "qa_model = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=qa_model_name,\n",
    "    tokenizer=qa_tokenizer,\n",
    "    device=-1    #CPU\n",
    ")\n",
    "\n",
    "MAX_TOKENS = 512\n",
    "\n",
    "# ----------------------------------\n",
    "# Utility Functions\n",
    "# ----------------------------------\n",
    "def trim_context(context, max_tokens=MAX_TOKENS):\n",
    "    tokens = qa_tokenizer.encode(context, truncation=True, max_length=max_tokens)\n",
    "    return qa_tokenizer.decode(tokens, skip_special_tokens=True)\n",
    "\n",
    "def normalize(vectors):\n",
    "    \"\"\"\n",
    "    Normalizing vectors(embeddings) to unit length for cosine similarity\n",
    "    \"\"\"\n",
    "    norms = np.linalg.norm(vectors, axis=1, keepdims=True)\n",
    "    norms[norms == 0] = 1e-10   #Avoiding zero division error\n",
    "    return vectors/norms\n",
    "\n",
    "# ----------------------------------\n",
    "# Helper Functions\n",
    "# ----------------------------------\n",
    "def extract_text(file_path):\n",
    "    \"\"\"\n",
    "    This function extracts text from a document (PDF, DOCX, or TXT).\n",
    "    It automatically detects the file type by extension.\n",
    "    \"\"\"\n",
    "    file_type = detect_file_type(file_path)\n",
    "    \n",
    "    if file_type == \".pdf\":         #Call the pdf extractor\n",
    "        return extract_text_from_pdf(file_path)\n",
    "\n",
    "    elif file_type == \".docx\":     #Call the docx extractor\n",
    "        return extract_text_from_docx(file_path)\n",
    "\n",
    "    elif file_type == \".txt\":      #Call the txt extractor\n",
    "        return extract_text_from_txt(file_path)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"❌Unsupported file type: '{ext}'\\n Only .pdf, .docx, or .txt files are supported\")\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Normalize and clean extracted text.\"\"\"\n",
    "    #Normalizing line breaks and spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)  #Replaces multiple spaces/newlines with one space\n",
    "\n",
    "    #Removing continuous dot leaders (e.g., \"Introduction ..... 3\")\n",
    "    text = re.sub(r'\\.{3,}\\s*\\d*', ' ', text)\n",
    "\n",
    "    #Removing spaced dot leaders\n",
    "    text = re.sub(r'(\\.\\s){2,}\\d*', ' ', text)\n",
    "\n",
    "    #Removing isolated page numbers (e.g., \"Page 2\", \"2\")\n",
    "    text = re.sub(r'\\bPage\\s*\\d+\\b', ' ', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\b\\d+\\b', ' ', text)     #Removing stray digits\n",
    "\n",
    "    #Removing non-ASCII characters\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)  #Removes non-ASCII characters\n",
    "  \n",
    "    #Normalizing multiple spaces again\n",
    "    text = re.sub(r'\\s{2,}', ' ', text)\n",
    "\n",
    "    #Trimming final whitespaces\n",
    "    text = text.strip()\n",
    "\n",
    "    #Return the cleaned text\n",
    "    return text\n",
    "\n",
    "\n",
    "def chunk_text(text, chunk_size=1000, overlap=100):\n",
    "    \"\"\"\n",
    "    This function splits text into overlapping chunks\n",
    "    :param text: Cleaned text.\n",
    "    :param chunk_size: Number of characters per chunk\n",
    "    :param overlap: Overlap between chunks to maintain context\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    text_length = len(text)\n",
    "\n",
    "    while start < text_length:\n",
    "        end = min(start + chunk_size, text_length)\n",
    "        chunk = text[start:end]\n",
    "        chunks.append(chunk)\n",
    "        start += chunk_size - overlap  #Redefining the start for the next iteration\n",
    "\n",
    "    return chunks\n",
    "\n",
    "# ----------------------------------\n",
    "# File Processing\n",
    "# ----------------------------------\n",
    "def process_uploaded_file(file_path):\n",
    "    \"\"\"\n",
    "    Save uploaded file, extract text, create chunks, and build embeddings + FAISS index.\n",
    "    \"\"\"\n",
    "    if file_path is None:\n",
    "        return \"⚠️ No file uploaded.\"\n",
    "\n",
    "    # Step 1: Extracting text from the document and clean it\n",
    "    text = extract_text(file_path)\n",
    "    cleaned_text = clean_text(text)\n",
    "    chunks = chunk_text(cleaned_text)\n",
    "\n",
    "    if len(chunks) == 0:\n",
    "        return \"⚠ No readable text found in this document.\"\n",
    "\n",
    "    # Step 2: Creating embeddings\n",
    "    print(\"🔍 Generating embeddings for uploaded document...\")   \n",
    "    embeddings = embedding_model.encode(chunks)\n",
    "\n",
    "    #Step 3: Normalizing embeddings (for cosine similarity)\n",
    "    embeddings_norm = normalize(np.array(embeddings, dtype=np.float32))\n",
    "\n",
    "    #Step 4: Building FAISS Index (Inner Product for Cosine Similarity)\n",
    "    index = faiss.IndexFlatIP(embeddings.shape[1])\n",
    "    index.add(embeddings_norm)\n",
    "\n",
    "    # Store in global state\n",
    "    state[\"index\"] = index\n",
    "    state[\"chunks\"] = chunks\n",
    "\n",
    "    return f\"✅ Document '{os.path.basename(file_path)}' processed and indexed successfully!\"\n",
    "    \n",
    "\n",
    "# ----------------------------------\n",
    "# QA Pipeline\n",
    "# ----------------------------------\n",
    "def answer_question_gradio(query, index, chunks, embedding_model, top_k=2):\n",
    "    \"\"\"Retrieves top_k relevant chunks and generates an answer\"\"\"\n",
    "    \n",
    "    #if state[\"index\"] is None or state[\"chunks\"] is None:\n",
    "        #return \"⚠️ Knowledge base not loaded. Please upload or load a document first.\"\n",
    "    \n",
    "    # Step 1: Encoding and normalizing the user query\n",
    "    query_embedding = embedding_model.encode([query])\n",
    "    query_embedding_norm = normalize(np.array(query_embedding, dtype=np.float32))\n",
    "\n",
    "    # Step 2: Searching FAISS index using cosine similarity and returning only relevant result\n",
    "    distances, indices = index.search(query_embedding_norm, top_k)\n",
    "    \n",
    "     # Step 3: Returning relevant chunks\n",
    "    #Converting to list for easier handling\n",
    "    scores = distances[0]\n",
    "    retrieved_indices = indices[0]\n",
    "\n",
    "    #Setting a similarity threshold\n",
    "    SIMILARITY_THRESHOLD = 0.11        #Lowered value for normalized embeddings\n",
    "\n",
    "    #Filtering out irrelevant chunks\n",
    "    relevant_chunks = [\n",
    "        chunks[i] for i, score in zip(retrieved_indices, scores)\n",
    "        if score >= SIMILARITY_THRESHOLD\n",
    "    ]\n",
    "\n",
    "    #Optional:\n",
    "    print(\"Similarity scores:\", scores)\n",
    "\n",
    "    #If no relevant chunk meets the threshold\n",
    "    if not relevant_chunks:\n",
    "        return \"🤔 I couldn't find relevant information in the uploaded document to answer the question.\"\n",
    "      \n",
    "    # Step 4: Concatenating retrieved chunks into context\n",
    "    context = \" \".join(relevant_chunks)\n",
    "        \n",
    "    # Step 5: Trimming context to fit model's token limit\n",
    "    trimmed_context = trim_context(context)\n",
    "       \n",
    "    # Step 6: Creating the prompt\n",
    "    prompt = (\n",
    "        f\"You are a helpful assistant. Use only the information in the context below to answer.\\n\"\n",
    "        f\"Context: {trimmed_context}\\n\\n\"\n",
    "        f\"Question: {query}\\n\\n\"\n",
    "        f\"Answer the question concisely based on the context above.\"\n",
    "        f\"If the answer cannot be found in the context, reply exactly with: \"\n",
    "        f\"'🤔 I couldn't find relevant information in the uploaded document to answer the question.'\"\n",
    "    )\n",
    "    \n",
    "    # Step 7: Generating an answer\n",
    "    response = qa_model(\n",
    "        prompt,\n",
    "        max_new_tokens=100,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        do_sample=True\n",
    "    )\n",
    "    \n",
    "    # Step 8: Returning the final answer (for Gradio)\n",
    "    answer = response[0][\"generated_text\"].strip()\n",
    "    return answer\n",
    "\n",
    "# ----------------------------------\n",
    "# Gradio UI Functions\n",
    "# ----------------------------------\n",
    "\n",
    "\n",
    "def ask_question_gradio(question):\n",
    "    \"\"\"Interface function for Gradio to answer user queries.\"\"\"\n",
    "    if state[\"index\"] is None or state[\"chunks\"] is None:\n",
    "        return \"⚠ Please upload and process a document first\"\n",
    "\n",
    "    return answer_question_gradio(\n",
    "        query=question, \n",
    "        index=state[\"index\"], \n",
    "        chunks=state[\"chunks\"],\n",
    "        embedding_model=embedding_model\n",
    "    )\n",
    "\n",
    "# ----------------------------------\n",
    "# Gradio Layout\n",
    "# ----------------------------------\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"<h2>📚 Mini RAG Chatbot</h2>\")\n",
    "    \n",
    "    with gr.Tab(\"Upload & Process Document\"):\n",
    "        file_input = gr.File(\n",
    "            label=\"Upload a PDF, DOCX, or TXT file\",\n",
    "            type=\"filepath\",\n",
    "            file_types=[\".pdf\", \".docx\", \".txt\"]\n",
    "        )\n",
    "        process_btn = gr.Button(\"Process Document\")\n",
    "        process_output = gr.Textbox(label=\"Status\", interactive=False)\n",
    "        process_btn.click(process_uploaded_file, inputs=file_input, outputs=process_output)\n",
    "\n",
    "    with gr.Tab(\"Ask Questions\"):\n",
    "        question_input = gr.Textbox(label=\"Enter your question here\")\n",
    "        ask_btn = gr.Button(\"💬Ask\")\n",
    "        answer_output = gr.Textbox(label=\"Answer\", interactive=False)\n",
    "        ask_btn.click(ask_question_gradio, inputs=question_input, outputs=answer_output)\n",
    "\n",
    "# ----------------------------------\n",
    "# Launch the app\n",
    "# ----------------------------------\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()  # Patching asyncio to allow nested event loops in notebooks\n",
    "\n",
    "demo.launch(\n",
    "    share=True,\n",
    "    inbrowser=True,\n",
    "    prevent_thread_lock=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (rag_env)",
   "language": "python",
   "name": "rag_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
